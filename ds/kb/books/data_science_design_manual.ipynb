{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Science Design Manual\n",
    "\n",
    "Notes by Tobias Reaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Fundamental principles of becoming a good data scientist:\n",
    "\n",
    "* Valuing doing the simple things right\n",
    "  * Understanding the application domain\n",
    "  * Cleaning and integrating relevant data sources\n",
    "  * Presenting your results clearly to others\n",
    "* Developing mathematical intuition\n",
    "  * Particularly statistics and linear algebra\n",
    "  * Why the concepts were developed, how they are useful, and when they work best\n",
    "* Think like a computer scientist, but act like a statistician"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Chapter 1: What is Data Science?\n",
    "\n",
    "### 1.2: Asking Interesting Questions from Data\n",
    "\n",
    "Good data scientists have wide-ranging interests. They read the newspaper every day to get a broader perspective on what is exciting. They understand that the world is an interesting place. Knowing a little something about everything equips them to play in other people's backyards. They are brave enough to get out of their comfort zones a bit, and driven to learn more once they get there.\n",
    "\n",
    "#### Baseball\n",
    "\n",
    "First example / exercise is baseball. Here are (interesting?) questions I came up with:\n",
    "\n",
    "- Who are the most expensive players, and how did they perform compared with the least expensive? Or compared with the median?\n",
    "- How much does a home run cost for the most/least expensive players?\n",
    "- Do height and weight determine the length of a player's career?\n",
    "- Are the most valuable players those who both bat and throw well / are well-rounded?\n",
    "- Do star players help a team win championships?\n",
    "\n",
    "Some interesting demographic ones:\n",
    "\n",
    "- How often do people return to live in the same place where they grew up?\n",
    "- Do lefties live longer than righties?\n",
    "\n",
    "#### IMDb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Chapter 2: Mathematical Preliminaries"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "### 2.1 Probability\n",
    "\n",
    "> Probability theory provides a formal framework for reasoning about the likelihood of events.\n",
    "\n",
    "- An experiment is a procedure yielding one of a set of possible outcomes\n",
    "  - On-going example: tossing two 6-sided dice, one red and one blue\n",
    "- A _sample space_ $S$ is the set of possible outcomes of an experiment\n",
    "  - In ex: there are 36 possible outcomes. \n",
    "- An _event_ $E$ is a specified subset of the outcomes of an experiment\n",
    "- THe _probability of an outcome_ $s$, denoted $p(s)$, is a number with two properties\n",
    "  - For each outcome $s$ in sample space $S$, $0 \\leq p(s) \\leq 1$\n",
    "  - The sum of probabilities of all outcomes adds to one\n",
    "- The _probability of an event_ $E$ is the sum of the probabilities of the outcomes of the experiment\n",
    "  - An easier method of calculating the probability is via the complement of $E$: $P(E) = 1 - P(\\bar{E})$\n",
    "- A _random variable_ $V$ is a numerical function on the outcomes of a probability space\n",
    "- The _expected value_ of a random variable $V$ defined on sample space $S$ is ...\n",
    "  - the probability of the event times its respective value, summed over all events\n",
    "\n",
    "$E(V) = \\sum p(s) \\cdot V(s)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.1.1 Probability vs. Statistics\n",
    "\n",
    "- Probability deals with predicting the likelihood of future events; theoretical math\n",
    "  - Probability theory enables us to find the consequences of a given ideal world\n",
    "- Statistics involves the analysis of the frequency of past events; applied math\n",
    "  - Statistical theory enables us to measure the extent to which our world is ideal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.1.2 Compound Events and Independence\n",
    "\n",
    "- Intersection\n",
    "  - The outcomes in common between both events $A$ and $B$ are the intersection: $A \\cap B$\n",
    "  - Written as $A \\cap B = A - (S - B)$\n",
    "- Union\n",
    "  - The outcomes in which either $A$ or $B$ appear are the union: $A \\cup B$\n",
    "- Events $A$ and $B$ are independent if and only if ${P(A \\cap B) = P(A) \\times P(B)}$\n",
    "  - prob of intersection of A and B is equal to prob of A times prob of B"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.1.3 Conditional Probability\n",
    "\n",
    "- Interested in the likelihood of event A as a function of some evidence B\n",
    "- The conditional probability of $A$ given $B$, $P(A \\mid B)$ is defined as\n",
    "\n",
    "$P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "Example:\n",
    "\n",
    "- Event $A$ is that at least one of the two dice be an even number\n",
    "- Event $B$ is the sum of the two dice is either a 7 or 11\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- $P(A \\mid B) = 1$ because any roll summing to odd must have one even and one odd number.\n",
    "- $A \\cap B = B$ - intersection b/w A and B is equal to B (A has more outcomes)\n",
    "- $A \\cap B = \\frac{9}{36}$ and $P(A) = \\frac{25}{36}$\n",
    "- $P(B \\mid A) = \\frac{9}{25}$\n",
    "\n",
    "> Primary tool to compute conditional probabilities is Bayes Theorem:\n",
    "\n",
    "$P(B \\mid A) = \\frac{P(A \\mid B)P(B)}{P(A)}$\n",
    "\n",
    "Using Bayes Theorem:\n",
    "\n",
    "$P(B \\mid A) = \\frac{1 \\cdot 9/36}{25/36} = 9/25$\n",
    "\n",
    "(same result as above.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.1.4 Probability Distributions\n",
    "\n",
    "- Random variables are numerical functions with values associated with probabilities of occurrence\n",
    "- Random variables can be represented by their _probability density function_, or pdf\n",
    "  - x-axis is range of values the variable can take on\n",
    "  - y-axis shows probability of that value\n",
    "- pdf plots are related to histograms of data frequency\n",
    "  - in this case, y-axis is the observed frequency of each value of x\n",
    "- histogram -> pdf: divide each bucket by the total frequency over all buckets\n",
    "  - the sum of the entries becomes 1, and we have a probability distribution\n",
    "- Random variables can also be visualized with a _cumulative density function_, or cdf\n",
    "  - the running sum of probabilities in the pdf\n",
    "  - reflects the probability that a value x is above the line\n",
    "- pdf and cdf contain the same information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "### 2.2 Descriptive Statistics\n",
    "\n",
    "- Provide methods of capturing properties of a dataset or sample\n",
    "  - aggregation as data reduction\n",
    "- Two main types\n",
    "  - Central tendency measures\n",
    "  - Variability measures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.2.1 Centrality Measures\n",
    "\n",
    "- Mean (arithmetic): sum the values and divide by the number of observations\n",
    "  - Good for characterizing symmetric distributions without outliers\n",
    "- Geometric mean: the nth root of the product of n values (multiply all values, then take the nth root)\n",
    "  - Always less than or equal to the arithmetic mean\n",
    "  - Very sensitive to values near zero (a single 0 destroys all meaning - like an outlier of infinity in arithmetic)\n",
    "  - Good for averaging ratios\n",
    "- Median: exact middle of a sorted array of values\n",
    "  - Must be an actual observation (unless taking the arithmetic mean of middle two values)\n",
    "  - Generally better for skewed distributions or data with outliers\n",
    "- Mode: most frequent element in the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.2.2 Variability Measures\n",
    "\n",
    "- Standard deviation is the most common\n",
    "  - Sum of squares differences between individual elements and the mean\n",
    "  - variance is stdev squared (i.e. they measure the same thing)\n",
    "    - variance is very sensitive to outliers\n",
    "- Population vs sample: divide by $n$ or $n - 1$?\n",
    "  - Full population: $n$\n",
    "  - Sample: $n - 1$\n",
    "  - Example:\n",
    "    - Sampling one point doesn't say anyhting about underlying variance of any population\n",
    "    - But it makes sense to say there is zero variance in X among the population of a one-person island\n",
    "  - For reasonable-sized datasets, they are going to be about the same, so it doesn't really matter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.2.3 Interpreting Variance\n",
    "\n",
    "- _Sampling errors_ happen when observations capture unrepresentative circumstances\n",
    "- _Measurement errors_ represent the limits of precision inherent in any sensing device\n",
    "- _Signal to noise ratio_: how much the observations measure the actual quantity rather than the data variance\n",
    "- Example: weighing yourself in the morning\n",
    "  - when you last ate = sampling error\n",
    "  - quality / age of the scale = measurement error\n",
    "  - changes in body mass = actual variation\n",
    "- Distressingly (for data scientists), much of what happens in the world is random fluctuations or arbitrary coincidence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.2.4 Characterizing Distributions\n",
    "\n",
    "- Central tendencies by themselves don't do much to descibe distributions\n",
    "- Combined with measures of variability, a decent job can be done describing any distribution\n",
    "- Best to always report both when characterizing a distribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "### 2.3 Correlation Analysis\n",
    "\n",
    "- x and y are correlated when x has some predictive power of y\n",
    "- Correlations around 0 are useless for predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.3.1 Correlation Coefficients: Pearson and Spearman Rank\n",
    "\n",
    "- Measure somewhat different things but both operate on scale of -1 to 1\n",
    "- Pearson Correlation Coefficient\n",
    "  - How well a linear predictor (line) can fit the data\n",
    "  - More prominent; shows overall the weight of points above and below the mean\n",
    "  - Can be relatively easily tricked to show zero correlation when there is obvious predictive power\n",
    "  - $r = ... = \\frac{Cov(X, Y)}{\\sigma (X) \\sigma (Y)}$\n",
    "  - $Cov(X, Y) = \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})$\n",
    "- Spearman Rank Correlation Coefficient\n",
    "  - Counts the number of pairs of input points that are out of order\n",
    "  - Gives high score to non-linear but monotonic functions\n",
    "  - Less sensitive to extreme outliers than Pearson"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.3.2 The Power and Significance of Correlation\n",
    "\n",
    "- \"if you want to fit your data with a straight line, best to sample only two points.\"\n",
    "- Strength of correlation, $R^2$: square of sample correlation coefficient, $r^2$\n",
    "  - Estimates the proportion of variance in Y explained by X in a simple linear regression\n",
    "    - OLS line, by its very nature, will result in a residual values dataset with mean of 0\n",
    "    - If totally uncorrelated, $V(y) \\approx V(r)$ - i.e. the fit contributes nothing\n",
    "- Statistical significance\n",
    "  - Depends on sample size $n$ and $r$\n",
    "  - Traditionally, a correlation of $n$ points is significant if $\\alpha \\leq 1/20 = 0.05$\n",
    "  - Not a very strong standard, as that significance can be somewhat easily achieved with large enough samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.3.3 Correlation Does Not Imply Causation!\n",
    "\n",
    "- Correlation implies causation is a common error in thinking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.3.4 Detecting Periodicities by Autocorrelation\n",
    "\n",
    "- Compare a sequence to itself\n",
    "- A peak at a shift of 7 days (and multples of) means there is a weekly periodicity\n",
    "- Important concept in predicting future events: use previous observations as features in a model\n",
    "- Autocorrelation function tends to be highest for short lags; long-term predictions are less accurate\n",
    "- Efficient algorithm based on the _fast Fourier transform (FFT)_ makes it possible to calculate autocorrelation functions for long sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "### 2.4 Logarithms\n",
    "\n",
    "- The logarithm is the inverse exponential function\n",
    "  - $y = b^x$, which can be rewritten as $x = log_b y$\n",
    "  - Same as $b^{log_b y} = y$\n",
    "- Logarithms grow at a very slow rate\n",
    "  - Associated with processes of repeated multiplication or division"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.4.1 Logarithms and Multiplying Probabilities\n",
    "\n",
    "- Logarithms are still very important, particularly in the multiplication of long chains of probabilities\n",
    "  - Probabilities are small numbers\n",
    "  - Multiplying probabilities yeild very small numbers governing the chances of very rare events\n",
    "- Computers don't deal with very small floating point numbers well, because of errors\n",
    "  - Summing the logarithms of probabilities is much more numerically stable than multiplying them\n",
    "- Logarithms of probabilities are all negative numbers except $log(1) = 0$\n",
    "  - Something to be aware of; watch out for negative symbols in strange places"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.4.2 Logarithms and Ratios\n",
    "\n",
    "- Doing things like averaging ratios is committing a statistical sin\n",
    "  - $200/100 = 200%$ above baseline; while $100/200 = 50%$ below\n",
    "  - Same magnitude difference, but wildly different results\n",
    "  - e.g. average the above changes, and it seems like an overall _increase_, though it should be 0\n",
    "    - $200/100 = 2$; $100/200 = 0.5$ - average them together: $\\frac{2.5}{2} = 1.25$\n",
    "  - Do the same thing with logs results in the correct answer\n",
    "    - $log_2 2 = 1$; $log_2 (1/2) = -1$\n",
    "- Always plot the logarithm of ratios, not the ratio values themselves"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.4.3 Logarithms and Normalizing Skewed Distributions\n",
    "\n",
    "- Logarithms are great at transforming skewed data into something more normally-distributed\n",
    "  - e.g. power law distributions\n",
    "- Sometimes logs are too drastic; can try square root\n",
    "  - Plot the frequency distributions to see what comes up with the most normal distribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.5 - 2.6\n",
    "\n",
    "- 2.5 is the war story about using logarithms to find more pronounced patterns in gene coding\n",
    "- 2.6 is the chapter notes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "### 2.7 Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> Probability\n",
    "\n",
    "#### 2-1\n",
    "\n",
    "Suppose that 80% of people like peanut butter, 89% like jelly, and 78% like both. Given that a randomly samples person likes peanut butter, what is the probability that she also likes jelly?\n",
    "\n",
    "$P(PB) = 0.80$, $P(J) = 0.89$, and $P(PB \\cap J) = 0.78$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7120000000000001\n"
     ]
    }
   ],
   "source": [
    "p_pb = 0.8\n",
    "p_j = 0.89\n",
    "p_both = 0.78\n",
    "\n",
    "# Dependent if not equal to 0.78\n",
    "print(p_pb * p_j)"
   ]
  },
  {
   "source": [
    "These events are not independent, because $P(PB \\cap J) \\neq P(PB) \\times P(J)$. Or, these events are dependent.\n",
    "\n",
    "Looking for the probability of liking jelly, given she likes pb. Using Bayes Theorem: $P(J \\mid PB) = \\frac{P(PB \\mid J)P(J)}{P(PB)}$\n",
    "\n",
    "First, we need the probability of pb given jelly: $P(PB \\mid J) = \\frac{P(PB \\cap J)}{P(J)} = \\frac{0.78}{0.89} = 0.8764$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8764044943820225"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Prob of pb given jelly\n",
    "p_pb_j = p_both / p_j\n",
    "p_pb_j"
   ]
  },
  {
   "source": [
    "Then, we have all the values needed for bayes theorem:\n",
    "\n",
    "$P(J \\mid PB) = \\frac{P(PB \\mid J)P(J)}{P(PB)} = \\frac{0.8764 \\cdot 0.89}{0.8} = 0.975$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Plug into bayes theorem\n",
    "p_j_pb = (p_pb_j * p_j) / p_pb\n",
    "p_j_pb"
   ]
  },
  {
   "source": [
    "#### 2-2\n",
    "\n",
    "Suppose that $P(A) = 0.3$ and $P(B) = 0.7$\n",
    "\n",
    "1. Can you compute $P(A and B)$ if you only know $P(A)$ and $P(B)$?\n",
    "\n",
    "Only if the events are independent. If they are independent, then $P(A \\cap B) = P(A) \\times P(B) = 0.3 * 0.7 = 0.21$\n",
    "\n",
    "2. Assuming that events $A$ and $B$ arise from independent random processes:\n",
    "- What is $P(A and B)$ / $P(A \\cap B)$?\n",
    "\n",
    "$P(A \\cap B) = P(A) \\times P(B) = 0.3 * 0.7 = 0.21$\n",
    "\n",
    "- What is $P(A or B)$ / $P(A \\cup B)$?\n",
    "\n",
    "$P(A \\cup B) = P(A) + P(B) = 1$\n",
    "\n",
    "- What is $P(A \\mid B)$?\n",
    "\n",
    "If the events are independent, the probability of A does not change at all given other information. So, $P(A \\mid B) = P(A) = 0.3$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2-3\n",
    "\n",
    "Consider a game where your score is the maximum value from two dice. Compute the probability of each event from {1, ..., 6}.\n",
    "\n",
    "I believe this is indicating I should calculate the probability of each of these events happening - my score equals:\n",
    "\n",
    "- 1 => 0\n",
    "- 2 => $\\frac{1}{36} = 0.0278$\n",
    "- 3 => {(1, 2), (2, 1)} => $\\frac{2}{36} = \\frac{1}{16} = 0.0625$\n",
    "- 4 => {(1, 3), (2, 2), (3, 1)} => $\\frac{3}{36} = \\frac{1}{12} = 0.083$\n",
    "- 5 => {(1, 4), (2, 3), (3, 2), (4, 1)} => $\\frac{4}{36} = \\frac{1}{9} = 0.11$\n",
    "- 6 => {(1, 5), (2, 4), (3, 3), (4, 2), (5, 1)} => $\\frac{5}{36} = 0.1389$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2-4\n",
    "\n",
    "Prove that the cumulative distribution function of the maximum of a pair of values drawn from random variable $X$ is the square of the original distribution function of $X$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2-5\n",
    "\n",
    "If two binary random variables $X$ and $Y$ are independent, are $\\bar{X}$ (the complement of $X$) and $Y$ also independent? Give a proof or a counterexample."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37664bitvelapipenvde09592071074af6a70ce3b1ce38af95",
   "display_name": "Python 3.7.6 64-bit ('vela': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "d279672dec97687973da8f2b23dbdcc76259fde61874b489061eca7e42b844dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}