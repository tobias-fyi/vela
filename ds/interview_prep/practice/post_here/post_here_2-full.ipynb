{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Here: The Subreddit Suggester\n",
    "\n",
    "### Notebook by: _Tobias Reaper_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notebook outline\n",
    "\n",
    "* [_Imports and Configuration_](#Imports-and-Configuration)\n",
    "* [Introduction](#Introduction)\n",
    "  * [The Problem](#The-Problem)\n",
    "  * [The Solution (The App)](#The-Solution)\n",
    "  * [My Role](#My-Role)\n",
    "* [The Data](#The-Data)\n",
    "  * [Wrangling](#Wrangling)\n",
    "  * [Exploration](#Exploration)\n",
    "* [Modeling](#Modeling)\n",
    "  * [Challenges](#Challenges)\n",
    "  * [Feature Selection](#Feature-Selection)\n",
    "  * [Vectorization](#Vectorization)\n",
    "  * [Baseline](#Baseline)\n",
    "  * [Multinomial Naive Bayes](#Multinomial-Naive-Bayes)\n",
    "* [Final Thoughts](#Final-Thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General imports === #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ML imports === #\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# === NLP Imports === #\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configure pandas display settings === #\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "\n",
    "Reddit is an expansive site. Anyone who has spent any significant amount of time on it knows what I mean. There is a subreddit for seemingly every topic anyone could ever want to discuss or even think about (and many that most do not want think about).\n",
    "\n",
    "Reddit is a powerful site; a tool for connecting and sharing information with like- or unlike-minded individuals around the world. When used well, it can be a very useful resource.\n",
    "\n",
    "On the other hand, the deluge of information that's constantly piling into the pages of can be overwhelming and lead to wasted time. As with any tool, it can be used for good or for not-so-good.\n",
    "\n",
    "A common problem that Redditors experience, particularly those who are relatively new to the site, is where to post content. Given that there are subreddits for just about everything, with wildly varying degrees of specificity it can be quite overwhelming trying to find the best place for each post.\n",
    "\n",
    "Just to illustrate the point, some subreddits get _weirdly_ specific. I won't go into the _really_ weird or NSFW, but here are some good examples of what I mean by specific:\n",
    "\n",
    "* [r/Borderporn](https://www.reddit.com/r/Borderporn/)\n",
    "* [r/BreadStapledtoTrees](https://www.reddit.com/r/BreadStapledToTrees/)\n",
    "* [r/birdswitharms](https://www.reddit.com/r/birdswitharms/)\n",
    "* [r/totallynotrobots](https://old.reddit.com/r/totallynotrobots)\n",
    "\n",
    "...need I go on? (If you're curious and/or want to be entertained indefinitely, here is a [thread](https://www.reddit.com/r/AskReddit/comments/dd49gw/what_are_some_really_really_weird_subreddits/) with these and much, much more.)\n",
    "\n",
    "Most of the time when a post is deemed irrelevant to a particular subreddit, it will simply be removed by moderators or a bot. However, depending on the subreddit and how welcoming they are to newbies, sometimes it can lead to very unfriendly responses and/or bans.\n",
    "\n",
    "So how does one go about deciding where to post or pose a question?\n",
    "\n",
    "Post Here aims to take the guesswork out of this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution\n",
    "\n",
    "The goal with the Post Here app, as mentioned, is to provide a tool that makes it quick and easy to find the most appropriate subreddits for any given post. A user would simply provide the title and text of the their prospective post and the app would provide the user with a list of subreddit recommendations.\n",
    "\n",
    "Recommendations are produced by a model attempts to predict which subreddit a given post would belong to. The model was built using Scikit-learn, and was trained on a large dataset of reddit posts. In order to serve the recommendations to the web app, an API was built using Flask and deployed to Heroku.\n",
    "\n",
    "The live version of the app is linked below.\n",
    "\n",
    "[Post Here: The Subreddit Suggester](https://github.com/tobias-fyi/post_here_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Role\n",
    "\n",
    "I worked on the Post Here app with a remote, interdisciplinary team of data scientists, machine learning engineers, and web developers. I was the sole machine learning engineer on the team, responsible for the entire process of building and training the machine learning models.\n",
    "\n",
    "The main challenge I ran into, which directed the iterative process, was scope and dimensionality management.\n",
    "\n",
    "At this point in my machine learning journey, this was one of the larger datasets that I'd taken on. Uncompressed, the dataset we used was over 800mb of mostly natural language text.\n",
    "\n",
    "One aspect of natural language processing to keep in mind with such a dataset is the curse of dimensionality. When processed, a natural language dataset of this size would likely fall prey to the curse of dimensionality and prove somewhat unwieldy without large amounts of processing power.\n",
    "\n",
    "I was forced to research and apply various methods of addressing this problem in order to fit the resulting models on the free Heroku Dyno (500mb) while preserving adequate performance.\n",
    "\n",
    "One important way I had to wrangle with scope management was in deciding how many classes to try and predict. The original dataset contains data for 1,000 subreddits. It was not within the scope of a a four-day project to build a classification model of a caliber that could accurately classify 1,000 classes.\n",
    "\n",
    "In the beginning, I did try to build a basic model trained on all 1,000 classes. But with the time and processing power I had, it proved to be untenable. In the end, I settled for a model that classified text into 200 subreddits with a test accuracy of over 90%.\n",
    "[review after re-validating]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we ended up using to train the recommendation system is called the [Reddit Self-Post Classification Task dataset](https://www.kaggle.com/mswarbrickjones/reddit-selfposts), available on Kaggle thanks to Evolution AI. The full dataset clocks in at over 800mb, containing 1,013,000 rows: 1,000 posts each from 1,013 subreddits.\n",
    "\n",
    "The data was posted to reddit between June 2016 and June 2018.\n",
    "\n",
    "[more info from the article]\n",
    "\n",
    "For more details on the dataset, refer to Evolution AI's [blog post](https://evolution.ai//blog/page/5/an-imagenet-like-text-classification-task-based-on-reddit-posts/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling\n",
    "\n",
    "As seems to be common with NLP projects, the process of wrangling the data was very much intertwined with the modeling process. Of course, this could be said about any machine learning project. However, I feel like it is particularly so in the case of NLP.\n",
    "\n",
    "Therefore, this section—the one dedicated to data wrangling only—will be rather brief and basic. I go into much more detail in the Modeling section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1013000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d8knd</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn't the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here's the story. I'm an independent developer who produces my ow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58mbft</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is he still chugging beers while talking about how sober he is?&lt;lb&gt;&lt;lb&gt;Edited to add: As an addict myself, anyone I know whose been an addict doesn't drin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We alwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' but has anyone used the Floodlight?  I already have the wire and existing bracket for the floodlight on the back of my house, but the problem is that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress results...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both, the \"small\" tests (including those parts of blend) make my temps shoot up to 100c+/throttling even at pure stock with MCE off instantaneously (I find...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             subreddit  \\\n",
       "0  6d8knd  talesfromtechsupport   \n",
       "1  58mbft               teenmom   \n",
       "2  8f73s7                Harley   \n",
       "3  6ti6re          ringdoorbell   \n",
       "4  77sxto                 intel   \n",
       "\n",
       "                                                     title  \\\n",
       "0                   Remember your command line switches...   \n",
       "1                          So what was Matt \"addicted\" to?   \n",
       "2                                           No Club Colors   \n",
       "3              Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress results...   \n",
       "\n",
       "                                                                                                                                                                                                  selftext  \n",
       "0  Hi there,  <lb>The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn't the right place...<lb><lb>Alright. Here's the story. I'm an independent developer who produces my ow...  \n",
       "1  Did he ever say what his addiction was or is he still chugging beers while talking about how sober he is?<lb><lb>Edited to add: As an addict myself, anyone I know whose been an addict doesn't drin...  \n",
       "2  Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We alwa...  \n",
       "3  I know this is a sub for the 'Ring Doorbell' but has anyone used the Floodlight?  I already have the wire and existing bracket for the floodlight on the back of my house, but the problem is that i...  \n",
       "4  Prime95 (regardless of version) and OCCT both, the \"small\" tests (including those parts of blend) make my temps shoot up to 100c+/throttling even at pure stock with MCE off instantaneously (I find...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load the dataset === #\n",
    "rspct = pd.read_csv(\"assets/data/rspct.tsv\", sep=\"\\t\")\n",
    "print(rspct.shape)\n",
    "rspct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nulls\n",
    "\n",
    "Kaggle says that 12% of the subreddit column is null. If that is indeed the case, they did not get read into the dataframe correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "subreddit    0\n",
       "title        0\n",
       "selftext     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Null values === #\n",
    "rspct.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['talesfromtechsupport', 'teenmom', 'Harley', ..., 'halo',\n",
       "       'gtaonline', 'mead'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Get list of subreddits === #\n",
    "subreddits = df1[\"subreddit\"].unique()\n",
    "subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Prune list of subreddits === #\n",
    "num_classes = 200\n",
    "sub_small = subreddits[:num_classes]\n",
    "sub_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === The list of subreddits === #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle page description:\n",
    "\n",
    "> We recommend splitting out the last 20% of the data as a test set (we have organised so that this is a random, stratified sample of all the data. In our experiments, we have been optimising for the precision-at-K metric for K = {1, 3, 5}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Split up dataset into train and test === #\n",
    "\n",
    "# First 80% is train; last 20% is test\n",
    "train, test = \n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 48,  87,  27,  76,  84,  59,  20, 111])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Encode the target using LabelEncoder === #\n",
    "\n",
    "# This process naively transforms each class of the target into a number\n",
    "le = LabelEncoder() # Instantiate a new encoder instance\n",
    "le.fit(y_train)  # Fit it on training label data\n",
    "\n",
    "# Transform both using the train-fit instance\n",
    "y_train = le.transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "y_train[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "...and Feature Selection\n",
    "\n",
    "* Large dataset = tons of text features\n",
    "* Dimensionality reduction techniques\n",
    "  * Chi^2 & SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "27ddcca6c2af541e94133222961a83c11f54208e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16032, 10000), (4009, 10000))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Feature Selection === #\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "selector = SelectKBest(chi2, 10000)\n",
    "\n",
    "selector.fit(X_train_sparse, y_train)\n",
    "\n",
    "X_train_select = selector.transform(X_train_sparse)\n",
    "X_test_select  = selector.transform(X_test_sparse)\n",
    "\n",
    "X_train_select.shape, X_test_select.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d31738daa5d0382761477f16e79d1800ac6f730"
   },
   "outputs": [],
   "source": [
    "# === Evaluate performance using precision-at-k === #\n",
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.argsort(y_pred, axis=1)\n",
    "    y_pred = y_pred[:, ::-1][:, :k]\n",
    "    arr = [y in s for y, s in zip(y_true, y_pred)]\n",
    "    return np.mean(arr)\n",
    "\n",
    "print('precision@1 =', np.mean(y_test == y_pred_rfc))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba_rfc, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba_rfc, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [],
   "source": [
    "# === Baseline RandomForest model === #\n",
    "rfc = RandomForestClassifier(max_depth=32, n_jobs=-1, n_estimators=200)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Naive Bayes model === #\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "nb.fit(X_train_select, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 = 0.786979296582689\n",
      "precision@3 = 0.8877525567473186\n",
      "precision@5 = 0.9156896981790971\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate precision at k === #\n",
    "print('precision@1 =', np.mean(y_test == y_pred))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope Management, Revisited\n",
    "\n",
    "[Potential improvements to the models here]\n",
    "\n",
    "* Classifying first into category, then by specific subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('vela': pipenv)",
   "language": "python",
   "name": "python37664bitvelapipenvde09592071074af6a70ce3b1ce38af95"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
