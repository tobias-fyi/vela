{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Here: The Subreddit Suggester\n",
    "\n",
    "### Notebook by: _Tobias Reaper_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notebook outline\n",
    "\n",
    "* [_Imports and Configuration_](#Imports-and-Configuration)\n",
    "* [Introduction](#Introduction)\n",
    "  * [The Problem](#The-Problem)\n",
    "  * [The Solution (The App)](#The-Solution)\n",
    "  * [My Role](#My-Role)\n",
    "* [The Data](#The-Data)\n",
    "  * [Wrangling](#Wrangling)\n",
    "  * [Exploration](#Exploration)\n",
    "* [Modeling](#Modeling)\n",
    "  * [Challenges](#Challenges)\n",
    "  * [Feature Selection](#Feature-Selection)\n",
    "  * [Vectorization](#Vectorization)\n",
    "  * [Baseline](#Baseline)\n",
    "  * [Multinomial Naive Bayes](#Multinomial-Naive-Bayes)\n",
    "* [Final Thoughts](#Final-Thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General imports === #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ML imports === #\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# === NLP Imports === #\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configure pandas display settings === #\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "\n",
    "Reddit is an expansive site. Anyone who has spent any significant amount of time on it knows what I mean. There is a subreddit for seemingly every topic anyone could ever want to discuss or even think about (and many that most do not want think about).\n",
    "\n",
    "Reddit is a powerful site. It is a tool for connecting and sharing information with like- or unlike-minded individuals around the world. When used well, it can help one accomplish things. \n",
    "\n",
    "On the other hand, the deluge of information that's constantly piling into the pages of can be overwhelming and lead to wasted time. As with any tool, it can be used for good or for not-so-good.\n",
    "\n",
    "A common problem that Redditors experience, particularly those who are relatively new to the site, is where to post content. Given that there are subreddits for just about everything, with wildly varying degrees of specificity it can be quite overwhelming trying to find the best place for each post.\n",
    "\n",
    "Just to illustrate the point, some subreddits get _weirdly_ specific. I won't go into the _really_ weird or NSFW, but here are some good examples of what I mean by specific:\n",
    "\n",
    "* [r/Borderporn](https://www.reddit.com/r/Borderporn/)\n",
    "* [r/BreadStapledtoTrees](https://www.reddit.com/r/BreadStapledToTrees/)\n",
    "* [r/birdswitharms](https://www.reddit.com/r/birdswitharms/)\n",
    "* [r/totallynotrobots](https://old.reddit.com/r/totallynotrobots)\n",
    "\n",
    "...need I go on? (If you're curious and/or want to be entertained indefinitely, here is a [thread](https://www.reddit.com/r/AskReddit/comments/dd49gw/what_are_some_really_really_weird_subreddits/) with these and much, much more.)\n",
    "\n",
    "Most of the time when a post is deemed irrelevant to a particular subreddit, it will simply be removed by moderators or a bot. However, depending on the subreddit and how welcoming they are to newbies, sometimes it can lead to very unfriendly responses and/or bans.\n",
    "\n",
    "So how does one go about deciding where to post or pose a question?\n",
    "\n",
    "Post Here aims to take the guesswork out of this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution\n",
    "\n",
    "The goal with the Post Here app, as mentioned, is to provide a tool that makes it quick and easy to find the most appropriate subreddits for any given post. A user would simply provide the title and text of the their prospective post and the app would provide the user with a list of subreddit recommendations.\n",
    "\n",
    "Recommendations would be provided by a model trained on a large dataset of reddit posts.\n",
    "\n",
    "The live version of the app is linked below.\n",
    "\n",
    "[Post Here: The Subreddit Suggester](https://github.com/tobias-fyi/post_here_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Role\n",
    "\n",
    "I worked as a machine learning engineer on the Post Here app with a remote interdisciplinary team of data scientists, machine learning engineers, and web developers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we ended up using to train the recommendation system is called the [Reddit Self-Post Classification Task dataset](https://www.kaggle.com/mswarbrickjones/reddit-selfposts), available on Kaggle thanks to Evolution AI. Here is a [blog post](https://evolution.ai//blog/page/5/an-imagenet-like-text-classification-task-based-on-reddit-posts/) about the dataset.\n",
    "\n",
    "The full dataset clocks in at over 800mb, and includes 1,013,000 rows: 1,000 posts each from 1,013 subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load the dataset === #\n",
    "rspct = pd.read_csv(\"assets/data/rspct.tsv\", sep=\"\\t\")\n",
    "print(rspct.shape)\n",
    "rspct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Null values === #\n",
    "rspct.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle page description:\n",
    "\n",
    "> We recommend splitting out the last 20% of the data as a test set (we have organised so that this is a random, stratified sample of all the data. In our experiments, we have been optimising for the precision-at-K metric for K = {1, 3, 5}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Split up dataset into train and test === #\n",
    "\n",
    "# First 80% is train; last 20% is test\n",
    "train, test = \n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 48,  87,  27,  76,  84,  59,  20, 111])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Encode the target using LabelEncoder === #\n",
    "\n",
    "# This process naively transforms each class of the target into a number\n",
    "le = LabelEncoder() # Instantiate a new encoder instance\n",
    "le.fit(y_train)  # Fit it on training label data\n",
    "\n",
    "# Transform both using the train-fit instance\n",
    "y_train = le.transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "y_train[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "* Large dataset = tons of text features\n",
    "* Dimensionality reduction techniques\n",
    "  * Chi^2 & SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "27ddcca6c2af541e94133222961a83c11f54208e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16032, 10000), (4009, 10000))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Feature Selection === #\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "selector = SelectKBest(chi2, 10000)\n",
    "\n",
    "selector.fit(X_train_sparse, y_train)\n",
    "\n",
    "X_train_select = selector.transform(X_train_sparse)\n",
    "X_test_select  = selector.transform(X_test_sparse)\n",
    "\n",
    "X_train_select.shape, X_test_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d31738daa5d0382761477f16e79d1800ac6f730"
   },
   "outputs": [],
   "source": [
    "# === Evaluate performance using precision-at-k === #\n",
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.argsort(y_pred, axis=1)\n",
    "    y_pred = y_pred[:, ::-1][:, :k]\n",
    "    arr = [y in s for y, s in zip(y_true, y_pred)]\n",
    "    return np.mean(arr)\n",
    "\n",
    "print('precision@1 =', np.mean(y_test == y_pred_rfc))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba_rfc, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba_rfc, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [],
   "source": [
    "# === Baseline RandomForest model === #\n",
    "rfc = RandomForestClassifier(max_depth=32, n_jobs=-1, n_estimators=200)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Naive Bayes model === #\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "nb.fit(X_train_select, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 = 0.786979296582689\n",
      "precision@3 = 0.8877525567473186\n",
      "precision@5 = 0.9156896981790971\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate precision at k === #\n",
    "print('precision@1 =', np.mean(y_test == y_pred))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('vela': pipenv)",
   "language": "python",
   "name": "python37664bitvelapipenvde09592071074af6a70ce3b1ce38af95"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
