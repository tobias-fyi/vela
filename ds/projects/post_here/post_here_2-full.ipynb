{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Here: The Subreddit Suggester\n",
    "\n",
    "## Small Model (limited dataset)\n",
    "\n",
    "### Notebook by: _Tobias Reaper_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notebook outline\n",
    "\n",
    "* [_Imports and Configuration_](#Imports-and-Configuration)\n",
    "* [Introduction](#Introduction)\n",
    "  * [The Problem](#The-Problem)\n",
    "  * [The Solution (The App)](#The-Solution)\n",
    "  * [My Role](#My-Role)\n",
    "* [The Data](#The-Data)\n",
    "  * [Wrangling](#Wrangling)\n",
    "  * [Exploration](#Exploration)\n",
    "* [Modeling](#Modeling)\n",
    "  * [Challenges](#Challenges)\n",
    "  * [Feature Selection](#Feature-Selection)\n",
    "  * [Vectorization](#Vectorization)\n",
    "  * [Baseline](#Baseline)\n",
    "  * [Multinomial Naive Bayes](#Multinomial-Naive-Bayes)\n",
    "* [Final Thoughts](#Final-Thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General imports === #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ML imports === #\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# === NLP Imports === #\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configure === #\n",
    "# Load spacy language model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Configure pandas display settings\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "# Set random seed\n",
    "seed = 92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "\n",
    "Reddit is an expansive site. Anyone who has spent any significant amount of time on it knows what I mean. There is a subreddit for seemingly every topic anyone could ever want to discuss or even think about (and many that most do not want think about).\n",
    "\n",
    "Reddit is a powerful site; a tool for connecting and sharing information with like- or unlike-minded individuals around the world. When used well, it can be a very useful resource.\n",
    "\n",
    "On the other hand, the deluge of information that's constantly piling into the pages of can be overwhelming and lead to wasted time. As with any tool, it can be used for good or for not-so-good.\n",
    "\n",
    "A common problem that Redditors experience, particularly those who are relatively new to the site, is where to post content. Given that there are subreddits for just about everything, with wildly varying degrees of specificity it can be quite overwhelming trying to find the best place for each post.\n",
    "\n",
    "Just to illustrate the point, some subreddits get _weirdly_ specific. I won't go into the _really_ weird or NSFW, but here are some good examples of what I mean by specific:\n",
    "\n",
    "* [r/Borderporn](https://www.reddit.com/r/Borderporn/)\n",
    "* [r/BreadStapledtoTrees](https://www.reddit.com/r/BreadStapledToTrees/)\n",
    "* [r/birdswitharms](https://www.reddit.com/r/birdswitharms/)\n",
    "* [r/totallynotrobots](https://old.reddit.com/r/totallynotrobots)\n",
    "\n",
    "...need I go on? (If you're curious and/or want to be entertained indefinitely, here is a [thread](https://www.reddit.com/r/AskReddit/comments/dd49gw/what_are_some_really_really_weird_subreddits/) with these and much, much more.)\n",
    "\n",
    "Most of the time when a post is deemed irrelevant to a particular subreddit, it will simply be removed by moderators or a bot. However, depending on the subreddit and how welcoming they are to newbies, sometimes it can lead to very unfriendly responses and/or bans.\n",
    "\n",
    "So how does one go about deciding where to post or pose a question?\n",
    "\n",
    "Post Here aims to take the guesswork out of this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution\n",
    "\n",
    "The goal with the Post Here app, as mentioned, is to provide a tool that makes it quick and easy to find the most appropriate subreddits for any given post. A user would simply provide the title and text of the their prospective post and the app would provide the user with a list of subreddit recommendations.\n",
    "\n",
    "Recommendations are produced by a model attempts to predict which subreddit a given post would belong to. The model was built using Scikit-learn, and was trained on a large dataset of reddit posts. In order to serve the recommendations to the web app, an API was built using Flask and deployed to Heroku.\n",
    "\n",
    "The live version of the app is linked below.\n",
    "\n",
    "[Post Here: The Subreddit Suggester](https://github.com/tobias-fyi/post_here_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Role\n",
    "\n",
    "I worked on the Post Here app with a remote, interdisciplinary team of data scientists, machine learning engineers, and web developers. I was the sole machine learning engineer on the team, responsible for the entire process of building and training the machine learning models.\n",
    "\n",
    "The main challenge I ran into, which directed the iterative process, was scope and dimensionality management.\n",
    "\n",
    "At this point in my machine learning journey, this was one of the larger datasets that I'd taken on. Uncompressed, the dataset we used was over 800mb of mostly natural language text.\n",
    "\n",
    "One aspect of natural language processing to keep in mind with such a dataset is the curse of dimensionality. When processed, a natural language dataset of this size would likely fall prey to the curse of dimensionality and prove somewhat unwieldy without large amounts of processing power.\n",
    "\n",
    "I was forced to research and apply various methods of addressing this problem in order to fit the resulting models on the free Heroku Dyno (500mb) while preserving adequate performance.\n",
    "\n",
    "One important way I had to wrangle with scope management was in deciding how many classes to try and predict. The original dataset contains data for 1,000 subreddits. It was not within the scope of a a four-day project to build a classification model of a caliber that could accurately classify 1,000 classes.\n",
    "\n",
    "In the beginning, I did try to build a basic model trained on all 1,000 classes. But with the time and processing power I had, it proved to be untenable. In the end, I settled for a model that classified text into 200 subreddits with a test accuracy of over 90%.\n",
    "[review after re-validating]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we ended up using to train the recommendation system is called the [Reddit Self-Post Classification Task dataset](https://www.kaggle.com/mswarbrickjones/reddit-selfposts), available on Kaggle thanks to Evolution AI. The full dataset clocks in at over 800mb, containing 1,013,000 rows: 1,000 posts each from 1,013 subreddits.\n",
    "\n",
    "The data was posted to reddit between June 2016 and June 2018.\n",
    "\n",
    "[more info from the article]\n",
    "\n",
    "For more details on the dataset, refer to Evolution AI's [blog post](https://evolution.ai//blog/page/5/an-imagenet-like-text-classification-task-based-on-reddit-posts/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling and Exploration\n",
    "\n",
    "As seems to be common with NLP projects, the process of wrangling the data was very much intertwined with the modeling process. Of course, this could be said about any machine learning project. However, I feel like it is particularly so in the case of NLP.\n",
    "\n",
    "Therefore, this section—the one dedicated to data wrangling only—will be rather brief and basic. I go into much more detail in the Modeling section.\n",
    "\n",
    "First, I needed to reduce the size of the dataset. I defined a subset of 12 categories which I thought were most relevant to the task at hand, and used that list to do the initial pruning. Those 12 categories left me with 305 unique subreddits and 305,000 rows. The list I used was as follows:\n",
    "\n",
    "* health\n",
    "* profession\n",
    "* electronics\n",
    "* hobby\n",
    "* writing/stories\n",
    "* advice/question\n",
    "* social_group\n",
    "* stem\n",
    "* parenting\n",
    "* books\n",
    "* finance/money\n",
    "* travel\n",
    "\n",
    "Next, I took a random sample of those 305,000 rows. The result was a dataset with 91,500 rows, now consisting of between 250 and 340 rows per subreddit. If I tried to use all of the features (tokens, or words) that resulted from this corpus, even in its reduced state, it would still result in a serialized vocabulary and/or model too large for our free Heroku Dyno. However, the features used in the final model can be chosen based on how useful they are for the classification.\n",
    "\n",
    "According to the dataset preview on Kaggle, there are quite a large number of missing values in each of the features—12%, 25%, and 39% of the subreddit, title, and selftext columns, respectively. However, I did not find any sign of those null values in the dataset nor mention of them in the dataset's companion blog post or article. I chocked it up to an error in the Kaggle preview.\n",
    "\n",
    "Finally, I went about doing some basic preprocessing to get the data ready for vectorization. As described in the description page on Kaggle, newline and tab characters were replaced with their HTML equivalents, `<lb>` and `<tab>`. I removed those and other HTML entities using a regular expression. I also concatenated the `title` and `selftext` features into a single text feature in order to process them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1013000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d8knd</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn't the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here's the story. I'm an independent developer who produces my ow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58mbft</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is he still chugging beers while talking about how sober he is?&lt;lb&gt;&lt;lb&gt;Edited to add: As an addict myself, anyone I know whose been an addict doesn't drin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We alwa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             subreddit                                   title  \\\n",
       "0  6d8knd  talesfromtechsupport  Remember your command line switches...   \n",
       "1  58mbft               teenmom         So what was Matt \"addicted\" to?   \n",
       "2  8f73s7                Harley                          No Club Colors   \n",
       "\n",
       "                                                                                                                                                                                                  selftext  \n",
       "0  Hi there,  <lb>The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn't the right place...<lb><lb>Alright. Here's the story. I'm an independent developer who produces my ow...  \n",
       "1  Did he ever say what his addiction was or is he still chugging beers while talking about how sober he is?<lb><lb>Edited to add: As an addict myself, anyone I know whose been an addict doesn't drin...  \n",
       "2  Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We alwa...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load the dataset === #\n",
    "rspct = pd.read_csv(\"assets/data/rspct.tsv\", sep=\"\\t\")\n",
    "print(rspct.shape)\n",
    "rspct.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nulls\n",
    "\n",
    "Kaggle says that 12%, 25%, and 39% of the subreddit, title, and selftext columns are null, respectively. If that is indeed the case, they did not get read into the dataframe correctly. However, it could be an error on Kaggle's part, seeing as there is no mention of these anywhere else in the description or blog post or article, nor sign of them during my explorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "subreddit    0\n",
       "title        0\n",
       "selftext     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Null values === #\n",
    "rspct.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess\n",
    "\n",
    "To prune the list of subreddits, I'll load in the `subreddit_info.csv` file, join, then choose a certain number of categories (category_1) to filter on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3394, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>whatsthatbook</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CasualConversation</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clairvoyantreadings</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecidingToBeBetter</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HelpMeFind</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit       category_1 category_2\n",
       "0        whatsthatbook  advice/question       book\n",
       "1   CasualConversation  advice/question      broad\n",
       "2  Clairvoyantreadings  advice/question      broad\n",
       "3   DecidingToBeBetter  advice/question      broad\n",
       "4           HelpMeFind  advice/question      broad"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load info === #\n",
    "info = pd.read_csv(\"assets/data/subreddit_info.csv\", usecols=[\"subreddit\", \"category_1\", \"category_2\"])\n",
    "print(info.shape)\n",
    "info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1013000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn't the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here's the story. I'm an independent developer who produces my ow...</td>\n",
       "      <td>writing/stories</td>\n",
       "      <td>tech support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>I work IT for a certain clothing company and they use iPod Touchs for scanning some items</td>\n",
       "      <td>[ME]- Thank you fro calling Store support, this is David. How may I help you?&lt;lb&gt;&lt;lb&gt;[Store]- Yeah, my iPod is frozen&lt;lb&gt;&lt;lb&gt;[ME]- Okay, can I have you hold down the power and the home button at t...</td>\n",
       "      <td>writing/stories</td>\n",
       "      <td>tech support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>It... It says right there on the screen...?</td>\n",
       "      <td>Hi guys!  &lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;LTL, FTP - all that jazz. Starting you off with a short one.&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;I'm the senior supporter at a smaller tech company with clients all over t...</td>\n",
       "      <td>writing/stories</td>\n",
       "      <td>tech support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>The computers not working. FIX IT NOW!</td>\n",
       "      <td>Hey there TFTS! This is my second time posting. I don't work for any tech support company, but I do have friends, family and teachers at school that have no idea how stuff works.&lt;lb&gt;&lt;lb&gt;This tale ...</td>\n",
       "      <td>writing/stories</td>\n",
       "      <td>tech support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>A Storm of Unreasonableness</td>\n",
       "      <td>Usual LTL, FTP. I have shared this story on a different site, but after reading TFTS for sometime I figured it'd belong here as well. &lt;lb&gt;&lt;lb&gt;This is from when I worked at a 3rd party call center ...</td>\n",
       "      <td>writing/stories</td>\n",
       "      <td>tech support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit  \\\n",
       "0  talesfromtechsupport   \n",
       "1  talesfromtechsupport   \n",
       "2  talesfromtechsupport   \n",
       "3  talesfromtechsupport   \n",
       "4  talesfromtechsupport   \n",
       "\n",
       "                                                                                       title  \\\n",
       "0                                                     Remember your command line switches...   \n",
       "1  I work IT for a certain clothing company and they use iPod Touchs for scanning some items   \n",
       "2                                                It... It says right there on the screen...?   \n",
       "3                                                     The computers not working. FIX IT NOW!   \n",
       "4                                                                A Storm of Unreasonableness   \n",
       "\n",
       "                                                                                                                                                                                                  selftext  \\\n",
       "0  Hi there,  <lb>The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn't the right place...<lb><lb>Alright. Here's the story. I'm an independent developer who produces my ow...   \n",
       "1  [ME]- Thank you fro calling Store support, this is David. How may I help you?<lb><lb>[Store]- Yeah, my iPod is frozen<lb><lb>[ME]- Okay, can I have you hold down the power and the home button at t...   \n",
       "2  Hi guys!  <lb><lb>&amp;nbsp;<lb><lb>LTL, FTP - all that jazz. Starting you off with a short one.<lb><lb>&amp;nbsp;<lb><lb>I'm the senior supporter at a smaller tech company with clients all over t...   \n",
       "3  Hey there TFTS! This is my second time posting. I don't work for any tech support company, but I do have friends, family and teachers at school that have no idea how stuff works.<lb><lb>This tale ...   \n",
       "4  Usual LTL, FTP. I have shared this story on a different site, but after reading TFTS for sometime I figured it'd belong here as well. <lb><lb>This is from when I worked at a 3rd party call center ...   \n",
       "\n",
       "        category_1    category_2  \n",
       "0  writing/stories  tech support  \n",
       "1  writing/stories  tech support  \n",
       "2  writing/stories  tech support  \n",
       "3  writing/stories  tech support  \n",
       "4  writing/stories  tech support  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Join the two dataframes === #\n",
    "rspct = pd.merge(rspct, info, on=\"subreddit\").drop(columns=[\"id\"])\n",
    "print(rspct.shape)\n",
    "rspct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit     0\n",
       "title         0\n",
       "selftext      0\n",
       "category_1    0\n",
       "category_2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Still no nulls === #\n",
    "rspct.isnull().sum()  # That's a good sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_game               100000\n",
       "tv_show                   68000\n",
       "health                    58000\n",
       "profession                56000\n",
       "software                  52000\n",
       "electronics               51000\n",
       "music                     43000\n",
       "sports                    40000\n",
       "sex/relationships         31000\n",
       "hobby                     30000\n",
       "geo                       29000\n",
       "crypto                    29000\n",
       "company/website           28000\n",
       "other                     27000\n",
       "anime/manga               26000\n",
       "drugs                     23000\n",
       "writing/stories           22000\n",
       "programming               21000\n",
       "arts                      21000\n",
       "autos                     20000\n",
       "advice/question           18000\n",
       "education                 17000\n",
       "animals                   17000\n",
       "social_group              16000\n",
       "politics/viewpoint        16000\n",
       "food/drink                15000\n",
       "card_game                 15000\n",
       "stem                      14000\n",
       "hardware/tools            14000\n",
       "religion/supernatural     13000\n",
       "parenting                 13000\n",
       "books                     12000\n",
       "appearance                11000\n",
       "finance/money             10000\n",
       "meta                       9000\n",
       "board_game                 9000\n",
       "rpg                        7000\n",
       "movies                     7000\n",
       "travel                     5000\n",
       "Name: category_1, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Look at categories === #\n",
    "rspct[\"category_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(305000, 5)\n",
      "Unique subreddits: 305\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn't the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here's the story. I'm an independent developer who produces my ow...</td>\n",
       "      <td>writing/stories</td>\n",
       "      <td>tech support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>I work IT for a certain clothing company and they use iPod Touchs for scanning some items</td>\n",
       "      <td>[ME]- Thank you fro calling Store support, this is David. How may I help you?&lt;lb&gt;&lt;lb&gt;[Store]- Yeah, my iPod is frozen&lt;lb&gt;&lt;lb&gt;[ME]- Okay, can I have you hold down the power and the home button at t...</td>\n",
       "      <td>writing/stories</td>\n",
       "      <td>tech support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit  \\\n",
       "0  talesfromtechsupport   \n",
       "1  talesfromtechsupport   \n",
       "\n",
       "                                                                                       title  \\\n",
       "0                                                     Remember your command line switches...   \n",
       "1  I work IT for a certain clothing company and they use iPod Touchs for scanning some items   \n",
       "\n",
       "                                                                                                                                                                                                  selftext  \\\n",
       "0  Hi there,  <lb>The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn't the right place...<lb><lb>Alright. Here's the story. I'm an independent developer who produces my ow...   \n",
       "1  [ME]- Thank you fro calling Store support, this is David. How may I help you?<lb><lb>[Store]- Yeah, my iPod is frozen<lb><lb>[ME]- Okay, can I have you hold down the power and the home button at t...   \n",
       "\n",
       "        category_1    category_2  \n",
       "0  writing/stories  tech support  \n",
       "1  writing/stories  tech support  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Define list of categories to keep === #\n",
    "keep_cats = [\n",
    "    \"health\",\n",
    "    \"profession\",\n",
    "    \"electronics\",\n",
    "    \"hobby\",\n",
    "    \"writing/stories\",\n",
    "    \"advice/question\",\n",
    "    \"social_group\",\n",
    "    \"stem\",\n",
    "    \"parenting\",\n",
    "    \"books\",\n",
    "    \"finance/money\",\n",
    "    \"travel\",\n",
    "]\n",
    "\n",
    "# === Prune dataset to above categories === #\n",
    "# Overwriting to save memory\n",
    "rspct = rspct[rspct[\"category_1\"].isin(keep_cats)]\n",
    "print(rspct.shape)\n",
    "print(\"Unique subreddits:\", len(rspct[\"subreddit\"].unique()))\n",
    "rspct.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91500, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>594781</th>\n",
       "      <td>stepparents</td>\n",
       "      <td>Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce</td>\n",
       "      <td>Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious relationship her AP (23M) whom she met and cheated on me with 6 mont...</td>\n",
       "      <td>parenting</td>\n",
       "      <td>step parenting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617757</th>\n",
       "      <td>bigseo</td>\n",
       "      <td>Do we raise our pricing?</td>\n",
       "      <td>I took a management role at an agency. We're way, way under $500/mo for SEO pricing - and I'm embarrassed to say that we're hurting for business. Seems to me that it's a struggle to get clients to...</td>\n",
       "      <td>profession</td>\n",
       "      <td>seo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642368</th>\n",
       "      <td>chemistry</td>\n",
       "      <td>Mac vs. PC?</td>\n",
       "      <td>Hello, all! I am currently a senior in high school and in the fall I will be going to SUNY Geneseo, majoring in chemistry and minoring in mathematics. &lt;lb&gt;&lt;lb&gt;Geneseo requires it’s students to get...</td>\n",
       "      <td>stem</td>\n",
       "      <td>chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325221</th>\n",
       "      <td>migraine</td>\n",
       "      <td>Beer as an aural abortive?</td>\n",
       "      <td>Hiya folks,&lt;lb&gt;&lt;lb&gt;I've been a migraine sufferer pretty much my whole life. For me intense auras, numbness, confusion, the inability to speak or see is BY FAR the worst aspect of the ordeal. When ...</td>\n",
       "      <td>health</td>\n",
       "      <td>migraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524939</th>\n",
       "      <td>MouseReview</td>\n",
       "      <td>Recommend office mouse</td>\n",
       "      <td>I was hoping you folks could help me out. Here's my situation and requirements:&lt;lb&gt;&lt;lb&gt;* I don't play games at all&lt;lb&gt;* Budget $30.00 or less&lt;lb&gt;* Shape as close to old Microsoft Intellimouse Opti...</td>\n",
       "      <td>electronics</td>\n",
       "      <td>computer mouse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit  \\\n",
       "594781  stepparents   \n",
       "617757       bigseo   \n",
       "642368    chemistry   \n",
       "325221     migraine   \n",
       "524939  MouseReview   \n",
       "\n",
       "                                                                       title  \\\n",
       "594781  Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce   \n",
       "617757                                              Do we raise our pricing?   \n",
       "642368                                                           Mac vs. PC?   \n",
       "325221                                            Beer as an aural abortive?   \n",
       "524939                                                Recommend office mouse   \n",
       "\n",
       "                                                                                                                                                                                                       selftext  \\\n",
       "594781  Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious relationship her AP (23M) whom she met and cheated on me with 6 mont...   \n",
       "617757  I took a management role at an agency. We're way, way under $500/mo for SEO pricing - and I'm embarrassed to say that we're hurting for business. Seems to me that it's a struggle to get clients to...   \n",
       "642368  Hello, all! I am currently a senior in high school and in the fall I will be going to SUNY Geneseo, majoring in chemistry and minoring in mathematics. <lb><lb>Geneseo requires it’s students to get...   \n",
       "325221  Hiya folks,<lb><lb>I've been a migraine sufferer pretty much my whole life. For me intense auras, numbness, confusion, the inability to speak or see is BY FAR the worst aspect of the ordeal. When ...   \n",
       "524939  I was hoping you folks could help me out. Here's my situation and requirements:<lb><lb>* I don't play games at all<lb>* Budget $30.00 or less<lb>* Shape as close to old Microsoft Intellimouse Opti...   \n",
       "\n",
       "         category_1      category_2  \n",
       "594781    parenting  step parenting  \n",
       "617757   profession             seo  \n",
       "642368         stem       chemistry  \n",
       "325221       health        migraine  \n",
       "524939  electronics  computer mouse  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Take a sample of that === #\n",
    "rspct = rspct.sample(frac=.3, random_state=seed)\n",
    "print(rspct.shape)\n",
    "rspct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91500, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>594781</th>\n",
       "      <td>stepparents</td>\n",
       "      <td>Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617757</th>\n",
       "      <td>bigseo</td>\n",
       "      <td>Do we raise our pricing? I took a management role at an agency. We're way, way under $500/mo for SEO pricing - and I'm embarrassed to say that we're hurting for business. Seems to me that it's a s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit  \\\n",
       "594781  stepparents   \n",
       "617757       bigseo   \n",
       "\n",
       "                                                                                                                                                                                                           text  \n",
       "594781  Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious...  \n",
       "617757  Do we raise our pricing? I took a management role at an agency. We're way, way under $500/mo for SEO pricing - and I'm embarrassed to say that we're hurting for business. Seems to me that it's a s...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Clean up a bit === #\n",
    "# Concatenate title and selftext\n",
    "rspct[\"text\"] = rspct[\"title\"] + \" \" + rspct[\"selftext\"]\n",
    "\n",
    "# Drop categories\n",
    "rspct = rspct.drop(columns=[\"category_1\", \"category_2\", \"title\", \"selftext\"])\n",
    "\n",
    "print(rspct.shape)\n",
    "rspct.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>594781</th>\n",
       "      <td>stepparents</td>\n",
       "      <td>Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617757</th>\n",
       "      <td>bigseo</td>\n",
       "      <td>Do we raise our pricing? I took a management role at an agency. We're way, way under $500/mo for SEO pricing - and I'm embarrassed to say that we're hurting for business. Seems to me that it's a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642368</th>\n",
       "      <td>chemistry</td>\n",
       "      <td>Mac vs. PC? Hello, all! I am currently a senior in high school and in the fall I will be going to SUNY Geneseo, majoring in chemistry and minoring in mathematics. Geneseo requires it’s students to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325221</th>\n",
       "      <td>migraine</td>\n",
       "      <td>Beer as an aural abortive? Hiya folks,I've been a migraine sufferer pretty much my whole life. For me intense auras, numbness, confusion, the inability to speak or see is BY FAR the worst aspect o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524939</th>\n",
       "      <td>MouseReview</td>\n",
       "      <td>Recommend office mouse I was hoping you folks could help me out. Here's my situation and requirements:* I don't play games at all* Budget $30.00 or less* Shape as close to old Microsoft Intellimou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit  \\\n",
       "594781  stepparents   \n",
       "617757       bigseo   \n",
       "642368    chemistry   \n",
       "325221     migraine   \n",
       "524939  MouseReview   \n",
       "\n",
       "                                                                                                                                                                                                           text  \n",
       "594781  Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious...  \n",
       "617757  Do we raise our pricing? I took a management role at an agency. We're way, way under $500/mo for SEO pricing - and I'm embarrassed to say that we're hurting for business. Seems to me that it's a s...  \n",
       "642368  Mac vs. PC? Hello, all! I am currently a senior in high school and in the fall I will be going to SUNY Geneseo, majoring in chemistry and minoring in mathematics. Geneseo requires it’s students to...  \n",
       "325221  Beer as an aural abortive? Hiya folks,I've been a migraine sufferer pretty much my whole life. For me intense auras, numbness, confusion, the inability to speak or see is BY FAR the worst aspect o...  \n",
       "524939  Recommend office mouse I was hoping you folks could help me out. Here's my situation and requirements:* I don't play games at all* Budget $30.00 or less* Shape as close to old Microsoft Intellimou...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Remove <lb>, <tab>, and other HTML entities === #\n",
    "# NOTE: takes a couple minutes to run\n",
    "rspct[\"text\"] = rspct[\"text\"].str.replace(\"(<lb>)*|(<tab>)*|(&amp;)*|(nbsp;)*\", \"\")\n",
    "rspct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stepparents</td>\n",
       "      <td>Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bigseo</td>\n",
       "      <td>Do we raise our pricing? I took a management role at an agency. We're way, way under $500/mo for SEO pricing - and I'm embarrassed to say that we're hurting for business. Seems to me that it's a s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit  \\\n",
       "0  stepparents   \n",
       "1       bigseo   \n",
       "\n",
       "                                                                                                                                                                                                      text  \n",
       "0  Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious...  \n",
       "1  Do we raise our pricing? I took a management role at an agency. We're way, way under $500/mo for SEO pricing - and I'm embarrassed to say that we're hurting for business. Seems to me that it's a s...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Reset the index === #\n",
    "rspct = rspct.reset_index(drop=True)\n",
    "rspct.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save pruned dataset to file === #\n",
    "rspct.to_csv(\"assets/data/rspct_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['stepparents', 'bigseo', 'chemistry', 'migraine', 'MouseReview',\n",
       "       'Malazan', 'Standup', 'preppers', 'Invisalign', 'whatsthisplant',\n",
       "       'CrohnsDisease', 'KingkillerChronicle', 'OccupationalTherapy',\n",
       "       'churning', 'Libraries', 'acting', 'eczema', 'Allergies',\n",
       "       'bigboobproblems', 'AskAnthropology', 'psychotherapy',\n",
       "       'WayfarersPub', 'synthesizers', 'StopGaming', 'stopsmoking',\n",
       "       'eroticauthors', 'amazonecho', 'TalesFromThePizzaGuy',\n",
       "       'rheumatoid', 'homestead', 'VoiceActing', 'FinancialCareers',\n",
       "       'Sleepparalysis', 'ProtectAndServe', 'short', 'Fibromyalgia',\n",
       "       'teaching', 'PlasticSurgery', 'insomnia', 'PLC', 'rapecounseling',\n",
       "       'peacecorps', 'paintball', 'autism', 'Nanny', 'Plumbing',\n",
       "       'Epilepsy', 'asmr', 'fatpeoplestories', 'Magic'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === List of subreddits === #\n",
    "subreddits = rspct[\"subreddit\"].unique()\n",
    "print(len(subreddits))\n",
    "subreddits[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dreams             340\n",
       "Gifts              337\n",
       "Cubers             333\n",
       "cassetteculture    333\n",
       "HFY                333\n",
       "                  ... \n",
       "foreignservice     265\n",
       "immigration        263\n",
       "WritingPrompts     263\n",
       "TryingForABaby     262\n",
       "Physics            250\n",
       "Name: subreddit, Length: 305, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rspct[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((73200, 2), (18300, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Split up dataset into train and test === #\n",
    "train, test = train_test_split(rspct, test_size=0.2)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73200,) (18300,)\n",
      "(73200,) (18300,)\n"
     ]
    }
   ],
   "source": [
    "# === Split out feature/target === #\n",
    "X_train = train[\"text\"]\n",
    "X_test = test[\"text\"]\n",
    "\n",
    "y_train = train[\"subreddit\"]\n",
    "y_test = test[\"subreddit\"]\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([233, 271,  89,  86, 278, 197, 201, 126])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Encode the target using LabelEncoder === #\n",
    "\n",
    "# This process naively transforms each class of the target into a number\n",
    "le = LabelEncoder() # Instantiate a new encoder instance\n",
    "le.fit(y_train)  # Fit it on training label data\n",
    "\n",
    "# Transform both using the trained instance\n",
    "y_train = le.transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "y_train[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Custom tokenizer function that removes stop words and punctuation, and reduces each token down to its lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    \"\"\"Simple version: extracts spacy lemmas and returns them as a list.\n",
    "    Only filters spacy stopwords and punctuation.\n",
    "    \"\"\"\n",
    "    doc = nlp(doc)\n",
    "    tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.is_stop == False) and (token.is_punct == False):\n",
    "            tokens.append(token.lemma_.strip().lower())\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Vectorize! === #\n",
    "\n",
    "# Extract features from the text data using bag-of-words method\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=100000,\n",
    "    min_df=5,\n",
    "#     max_df=.98,\n",
    "    ngram_range=(1,2),\n",
    "#     tokenizer=tokenize\n",
    "#     stop_words=nlp.Defaults.stop_words,  # Use spacy's stop words\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the feature column to create vocab (doc-term matrix)\n",
    "vocab = tfidf.fit(X_train)\n",
    "\n",
    "# Get sparse document-term matrices\n",
    "X_train_sparse = vocab.transform(X_train)\n",
    "X_test_sparse = vocab.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<73200x100000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5332578 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "As mentioned previously, the size of the corpus means the dimensionality of the featureset after vectorization will be very high. In fact, I passed in  100,000 as the maximum number of features to the vectorizer. It is generally not good practice to have a larger number of features (100,000) than observations (91,500).\n",
    "\n",
    "To reduce it down from that 100,000, I used a process called select k best that does exactly what it sounds like: selects a certain number of the best features. The key aspect of this process is how to measure the value of the features; how to find which ones are the \"best\". The scoring function I used in this case is called [ch2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html) (chi-squared).\n",
    "\n",
    "This function calculates chi-squared statistics between each feature and the target, measuring the dependence, or correlation, between them. The intuition here is that features which are more correlated with the target are more likely to be useful to the model.\n",
    "\n",
    "I played around with some different values for the maximum number of features to be selected. Ultimately, I was once again limited by the size of the free Heroku Dyno, and settled on 10,000. This allowed the deployment to go smoothly while retaining enough information for the model to have adequate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "27ddcca6c2af541e94133222961a83c11f54208e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((73200, 10000), (18300, 10000))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Feature Selection === #\n",
    "selector = SelectKBest(chi2, k=10000)\n",
    "\n",
    "selector.fit(X_train_sparse, y_train)\n",
    "\n",
    "X_train_select = selector.transform(X_train_sparse)\n",
    "X_test_select  = selector.transform(X_test_sparse)\n",
    "\n",
    "X_train_select.shape, X_test_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<73200x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1272777 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation\n",
    "\n",
    "In this case, the model has a target that it is attempting to predict—a supervised problem. Therefore, the performance can be measured on a validation and test set.\n",
    "\n",
    "To test out the recommendations I picked some posts and put them through the prediction pipeline to see what kinds of subreddits were getting recommended. For the most part, the predictions were decent. Sometimes it would fall over...[put some examples here]\n",
    "\n",
    "The Kaggle page description:\n",
    "\n",
    "> In our experiments, we have been optimising for the precision-at-K metric for K = {1, 3, 5}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "\n",
    "For the baseline model, I decided to go with a basic random forest.\n",
    "\n",
    "Model performance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "8d31738daa5d0382761477f16e79d1800ac6f730"
   },
   "outputs": [],
   "source": [
    "# === Evaluate performance using precision-at-k === #\n",
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.argsort(y_pred, axis=1)\n",
    "    y_pred = y_pred[:, ::-1][:, :k]\n",
    "    arr = [y in s for y, s in zip(y_true, y_pred)]\n",
    "    return np.mean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=32, n_estimators=200, n_jobs=-1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Baseline RandomForest model === #\n",
    "rfc = RandomForestClassifier(max_depth=32, n_jobs=-1, n_estimators=200)\n",
    "rfc.fit(X_train_select, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([135, 302, 274, 302, 129, 195,  44, 137,  70, 199])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Create predictions on test feature === #\n",
    "y_pred_proba_rfc = rfc.predict_proba(X_test_select)\n",
    "\n",
    "# === For each prediction, find the index with the highest probability === #\n",
    "y_pred_rfc = np.argmax(y_pred_proba_rfc, axis=1)\n",
    "y_pred_rfc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 = 0.5002185792349727\n",
      "precision@3 = 0.5840983606557377\n",
      "precision@5 = 0.6060655737704918\n"
     ]
    }
   ],
   "source": [
    "print('precision@1 =', np.mean(y_test == y_pred_rfc))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba_rfc, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba_rfc, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes\n",
    "\n",
    "Multinomial naive Bayes is one of two classic naive Bayes models used for text classification. It is a probabilistic learning method for multinomially distributed data.\n",
    "\n",
    "Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Naive Bayes model === #\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "nb.fit(X_train_select, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([135, 171, 273, 214, 129, 260, 250, 263,  70, 124])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Create predictions on test feature === #\n",
    "y_pred_proba = nb.predict_proba(X_test_select)\n",
    "\n",
    "# === For each pred, find index with highest proba === #\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 = 0.7566120218579235\n",
      "precision@3 = 0.8753551912568306\n",
      "precision@5 = 0.9073770491803279\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate precision at k === #\n",
    "print('precision@1 =', np.mean(y_test == y_pred))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations\n",
    "\n",
    "The API should return a list of recommendations, not a single prediction. To accomplish this, I wrote a function that returns the top 5 most likely subreddits and their respective probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function to serve predictions === #\n",
    "# The main functionality of the predict API endpoint\n",
    "\n",
    "def predict(title: str, submission_text: str, return_count: int = 5):\n",
    "    \"\"\"\n",
    "    Serve subreddit predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : string\n",
    "        Title of post.\n",
    "    submission_text : string\n",
    "        Selftext that needs a home.\n",
    "    return_count    : integer\n",
    "        The desired number of recommendations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Python dictionary formatted as follows:\n",
    "        [{'subreddit': 'PLC', 'proba': 0.014454},\n",
    "         ...\n",
    "         {'subreddit': 'Rowing', 'proba': 0.005206}]\n",
    "    \"\"\"\n",
    "    # Concatenate title and post text\n",
    "    fulltext = str(title) + str(submission_text)\n",
    "    # Vectorize the post -> sparse doc-term matrix\n",
    "    post_sparse = vocab.transform([fulltext])\n",
    "    # Feature selection\n",
    "    post_select = selector.transform(post_sparse)\n",
    "    # Generate predicted probabilities from trained model\n",
    "    proba = nb.predict_proba(post_select)\n",
    "    # Wrangle into correct format\n",
    "    proba_dict = (pd\n",
    "                .DataFrame(proba, columns=[le.classes_])  # Classes as column names\n",
    "                .T  # Transpose so column names become index\n",
    "                .reset_index()  # Pull out index into a column\n",
    "                .rename(columns={\"level_0\": \"name\", 0: \"proba\"})  # Rename for aesthetics\n",
    "                .sort_values(by=\"proba\", ascending=False)  # Sort by probability\n",
    "                .iloc[:return_count]  # n-top predictions to serve\n",
    "                .to_dict(orient=\"records\")\n",
    "               )\n",
    "    proba_json = {\"predictions\": proba_dict}\n",
    "    \n",
    "    return proba_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'name': 'GERD', 'proba': 0.00990604371082869},\n",
       "  {'name': 'misophonia', 'proba': 0.009255419029406112},\n",
       "  {'name': 'AskAnthropology', 'proba': 0.008865383231338406},\n",
       "  {'name': 'fatpeoplestories', 'proba': 0.008636894366240455},\n",
       "  {'name': 'emetophobia', 'proba': 0.008542160336507437}]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_science = \"\"\"Is there an evolutionary benefit to eating spicy food that lead to consumption across numerous cultures throughout history? Or do humans just like the sensation?\"\"\"\n",
    "\n",
    "post_science = \"\"\"I love spicy food and have done ever since I tried it. By spicy I mean HOT, like chilli peppers (we say spicy in England, I don't mean to state the obvious I'm just not sure if that's a global term and I've assumed too much before). I love a vast array of spicy foods from all around the world. I was just wondering if there was some evolutionary basis as to why spicy food managed to become some widely consumed historically. Though there seem to\n",
    "\n",
    "It way well be that we just like a tingly mouth, the simple things in life.\"\"\"\n",
    "\n",
    "science_recs = predict(title_science, post_science)\n",
    "science_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'name': 'lego', 'proba': 0.009030090934930563},\n",
       "  {'name': 'vandwellers', 'proba': 0.0077818300061768255},\n",
       "  {'name': 'Luthier', 'proba': 0.006943420255904971},\n",
       "  {'name': 'rccars', 'proba': 0.006840724486847484},\n",
       "  {'name': 'cosplay', 'proba': 0.006534249663711068},\n",
       "  {'name': 'fightsticks', 'proba': 0.006515524774501326},\n",
       "  {'name': 'Machinists', 'proba': 0.006406856199114954},\n",
       "  {'name': 'cade', 'proba': 0.0063971277684109805},\n",
       "  {'name': 'MechanicalKeyboards', 'proba': 0.006307983344624362},\n",
       "  {'name': 'robotics', 'proba': 0.00599901604666438}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Test post from r/buildmeapc === #\n",
    "\n",
    "title_pc = \"\"\"Looking for help with a build\"\"\"\n",
    "\n",
    "post_pc = \"\"\"I posted my wants for my build about 2 months ago. Ordered them and when I went to build it I was soooooo lost. It took 3 days to put things together because I was afraid I would break something when I finally got the parts together it wouldn’t start, I was so defeated. With virtually replacing everything yesterday it finally booted and I couldn’t be more excited!\"\"\"\n",
    "\n",
    "post_pc_recs = predict(title_pc, post_pc, 10)\n",
    "post_pc_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'name': 'cscareerquestions', 'proba': 0.4090955393654394},\n",
       "  {'name': 'devops', 'proba': 0.02566286765112885},\n",
       "  {'name': 'interviews', 'proba': 0.02562100876400959},\n",
       "  {'name': 'resumes', 'proba': 0.024405441404559483},\n",
       "  {'name': 'datascience', 'proba': 0.023768874262813995}]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Example post from 'r/learnprogramming' === #\n",
    "\n",
    "post_title = \"\"\"What to do about java vs javascript\"\"\"\n",
    "\n",
    "post = \"\"\"I am a new grad looking for a job and currently in the process with a company for a junior backend engineer role. I was under the impression that the position was Javascript but instead it is actually Java. My general programming and \"leet code\" skills are pretty good, but my understanding of Java is pretty shallow. How can I use the next three days to best improve my general Java knowledge? Most resources on the web seem to be targeting complete beginners. Maybe a book I can skim through in the next few days?\n",
    "\n",
    "Edit:\n",
    "\n",
    "A lot of people are saying \"the company is a sinking ship don't even go to the interview\". I just want to add that the position was always for a \"junior backend engineer\". This company uses multiple languages and the recruiter just told me the incorrect language for the specific team I'm interviewing for. I'm sure they're mainly interested in seeing my understanding of good backend principles and software design, it's not a senior lead Java position.\"\"\"\n",
    "\n",
    "# === Test out the function === #\n",
    "post_pred = predict(post_title, post)  # Default is 5 results\n",
    "post_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'name': 'suggestmeabook', 'proba': 0.3762950415068399},\n",
       "  {'name': 'writing', 'proba': 0.1661331608591028},\n",
       "  {'name': 'whatsthatbook', 'proba': 0.06101540341494861},\n",
       "  {'name': 'eroticauthors', 'proba': 0.04492331535382657},\n",
       "  {'name': 'ComicBookCollabs', 'proba': 0.024625040605292445},\n",
       "  {'name': 'TheDarkTower', 'proba': 0.019166284059049136},\n",
       "  {'name': 'Malazan', 'proba': 0.01742526148901986},\n",
       "  {'name': 'DestructiveReaders', 'proba': 0.01740848101764242},\n",
       "  {'name': 'WoT', 'proba': 0.013847077414874202},\n",
       "  {'name': 'WritingPrompts', 'proba': 0.007749859890078947}]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Test it out with another dummy post === #\n",
    "\n",
    "title_book = \"Looking for books with great plot twists\"\n",
    "\n",
    "# This one comes from r/suggestmeabook\n",
    "post2 = \"\"\"I've been dreaming about writing my own stort story for a while but I want to give it an unexpected ending. I've read lots of books, but none of them had the plot twist I want. I want to read books with the best plot twists, so that I can analyze what makes a good plot twist and write my own story based on that points. I don't like romance novels and I mostly enjoy sci-fi or historical books but anything beside romance novels would work for me, it doesn't have to be my type of novel. I'm open to experience after all. I need your help guys. Thanks in advance.\"\"\"\n",
    "\n",
    "# === This time with 10 results === #\n",
    "post2_pred = predict(title_book, post2, 10)\n",
    "post2_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model deployment\n",
    "\n",
    "As mentioned, the model, vocab, and feature selector were all serialized using Python's pickle module. In the Flask app, the pickled objects are loaded and ready for use, just like that.\n",
    "\n",
    "I will go over the details of how the Flask app was set up in a separate blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope Management, Revisited\n",
    "\n",
    "[Potential improvements to the models here]\n",
    "\n",
    "* Tune hyperparameters\n",
    "* Deploying larger model to AWS\n",
    "* Classifying first into category, then by specific subreddit\n",
    "* Using more data for more subreddits up front, then reducing the number of features\n",
    "  * TODO: train a model with the full dataset in separate notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('vela': pipenv)",
   "language": "python",
   "name": "python37664bitvelapipenvde09592071074af6a70ce3b1ce38af95"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
